{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2575423e-4bc4-4595-bf0c-f4242d0bbf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65ad76a-84db-4b4c-b751-e0c0cae1527f",
   "metadata": {},
   "source": [
    "# Конфигурации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c344f1db-4486-4b10-a3ef-831ed0ba783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "  face_landmarks_list = detection_result.face_landmarks\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected faces to visualize.\n",
    "  for idx in range(len(face_landmarks_list)):\n",
    "    face_landmarks = face_landmarks_list[idx]\n",
    "\n",
    "    # Draw the face landmarks.\n",
    "    face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    face_landmarks_proto.landmark.extend([\n",
    "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in face_landmarks\n",
    "    ])\n",
    "\n",
    "    mp.solutions.drawing_utils.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks_proto,\n",
    "        connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp.solutions.drawing_styles\n",
    "        .get_default_face_mesh_tesselation_style())\n",
    "    mp.solutions.drawing_utils.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks_proto,\n",
    "        connections=mp.solutions.face_mesh.FACEMESH_IRISES,\n",
    "          landmark_drawing_spec=None,\n",
    "          connection_drawing_spec=mp.solutions.drawing_styles\n",
    "          .get_default_face_mesh_iris_connections_style())\n",
    "\n",
    "  return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dbe39cf-d03a-4fb2-bcb4-a24765763e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"face_landmarker.task\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12edad8d-bbc4-43b2-8d59-e300281dcdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "W20230915 13:46:47.543543 15965712 face_landmarker_graph.cc:168] Face blendshape model contains CPU only ops. Sets FaceBlendshapesGraph acceleration to Xnnpack.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "BaseOptions = mp.tasks.BaseOptions\n",
    "FaceLandmarker = mp.tasks.vision.FaceLandmarker\n",
    "FaceLandmarkerOptions = mp.tasks.vision.FaceLandmarkerOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "options = FaceLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.IMAGE)\n",
    "\n",
    "detector = FaceLandmarker.create_from_options(options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfba0779-ecbe-474e-9dd5-38b12c91f981",
   "metadata": {},
   "source": [
    "# Тестирование в реальном времени постороение точек на лице"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9f43e26-2eec-4548-ac76-5f8fa5a3b1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, image = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "    face_landmarker_result = detector.detect(mp_image)\n",
    "    annotated_image = draw_landmarks_on_image(image, face_landmarker_result)\n",
    "\n",
    "    cv2.imshow('frame', annotated_image)\n",
    "    \n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf77c11-26fb-4e43-b29b-ad80f1c0d77f",
   "metadata": {},
   "source": [
    "# Создание датаеста с эмоциями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40f8b68e-32b4-4e1a-95e3-0f61deea4a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed emotion:angry\n",
      "completed emotion:disgust\n",
      "completed emotion:fear\n",
      "completed emotion:happy\n",
      "completed emotion:neutral\n",
      "completed emotion:sad\n",
      "completed emotion:surprise\n",
      "Completed all emotion\n"
     ]
    }
   ],
   "source": [
    "directory_image_path = \"emotion-dataset\"\n",
    "\n",
    "emotion = [\"angry\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n",
    "emotion_cords_dataset = list()\n",
    "for dir_emotion in emotion:\n",
    "    dir_emotion_path = f\"{directory_image_path}/{dir_emotion}\"\n",
    "    \n",
    "    file_names = [file for file in os.listdir(dir_emotion_path) if os.path.isfile(os.path.join(dir_emotion_path, file))]\n",
    "\n",
    "    data_cord_emotion = list()\n",
    "\n",
    "    for image in file_names:\n",
    "        img = cv2.imread(f\"{dir_emotion_path}/{image}\")\n",
    "        \n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=img)\n",
    "        face_landmarker_result = detector.detect(mp_image)\n",
    "        if len(face_landmarker_result.face_landmarks) == 0:\n",
    "            continue\n",
    "        face_landmarker_result = face_landmarker_result.face_landmarks[0]\n",
    "        \n",
    "        cords_image = list(map(lambda landmark_xyz: [landmark_xyz.x, landmark_xyz.y, landmark_xyz.z], face_landmarker_result))\n",
    "\n",
    "        data_cord_emotion.append(cords_image)\n",
    "    emotion_cords_dataset.append(data_cord_emotion)\n",
    "    \n",
    "    print(f'completed emotion:{dir_emotion}')\n",
    "print('Completed all emotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb04c791-ea3d-4d38-b66d-52f8c005bd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_emotion_cords_dataset = emotion_cords_dataset.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b16d8b-d2ce-40e7-86ff-99c3a7623336",
   "metadata": {},
   "source": [
    "# Сохраниение numpy с [460, 460, 460, 460, 460, 460, 460]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8581ed9b-f5c7-4c71-9d98-c663b16a59c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4187, 460, 4349, 8503, 5854, 4994, 3667]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(i) for i in save_emotion_cords_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fbd58f80-8d70-4321-a107-1d40291a62d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_emotion_cords_dataset = list()\n",
    "for enotion in save_emotion_cords_dataset:\n",
    "    \n",
    "    enotion_copy = enotion.copy()\n",
    "    np.random.shuffle(enotion_copy)\n",
    "\n",
    "    limited_emotion_cords_dataset.append(enotion_copy[:460])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b7a69923-8909-4313-85c5-423d42be03b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[478, 478, 478, 478, 478, 478, 478]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(i) for i in limited_emotion_cords_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "015a5c0f-ac39-4e44-ab4c-9bb843fb24f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_emotion_cords_dataset = np.array(limited_emotion_cords_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2ffd8864-276b-45ee-8227-920f9e3dca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('emotion_cords_dataset.npy', np.array(limited_emotion_cords_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297234e2-ec21-4379-8a14-5090dfaab7ce",
   "metadata": {},
   "source": [
    "# Сохраниение list с [1000, 460, 1000, 1000, 1000, 1000, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c6ee8af-46d6-4195-ae36-c2a70716c46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_emotion_cords_dataset = emotion_cords_dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0b55e04-737b-4dff-b57c-2016e14d98bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4187, 460, 4349, 8503, 5854, 4994, 3667]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(i) for i in save_emotion_cords_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fda66783-ad37-44a6-b997-947a92e15573",
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_emotion_cords_dataset = list()\n",
    "for enotion in save_emotion_cords_dataset:\n",
    "    \n",
    "    enotion_copy = enotion.copy()\n",
    "    np.random.shuffle(enotion_copy)\n",
    "\n",
    "    limited_emotion_cords_dataset.append(enotion_copy[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "787ac9d7-7944-4c9a-a29e-fec2ccd9c983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1000, 460, 1000, 1000, 1000, 1000, 1000]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(i) for i in limited_emotion_cords_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27bdf5cd-4302-43f6-a627-8ece5f425b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('emotion_cords_dataset.pkl', 'wb') as file:\n",
    "    pickle.dump(limited_emotion_cords_dataset, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
